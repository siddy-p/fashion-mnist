{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9d4325ad",
      "metadata": {
        "id": "9d4325ad"
      },
      "source": [
        "# Your Details"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "633d99a8",
      "metadata": {
        "id": "633d99a8"
      },
      "source": [
        "Your Name: Siddartha Sandeep Peddada"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50b8d888",
      "metadata": {
        "id": "50b8d888"
      },
      "source": [
        "Your ID Number: 24192929"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34d67540",
      "metadata": {
        "id": "34d67540"
      },
      "source": [
        "# Etivity Part 2: Quantizing a TensorFlow/Keras Model\n",
        "\n",
        "\n",
        "* Understand Quantizations in TensorFlow\n",
        "* Quantize a CNN using the TensorFlow Model optimisation framework\n",
        "* Analyse the model perfromance\n",
        "* Results analysis\n",
        "\n",
        "### Let's get started!\n",
        "\n",
        "    [1] Import data dependencies\n",
        "    [2] Generate a TensorFlow/keras CNN model for the Fashion MNIST dataset\n",
        "    [3] Convert model to TF Lite model\n",
        "    [4] Perform Post Training Quantization (PTQ) to generate TF Lite model for:\n",
        "        (a) PTQ using Float 16 Quantization\n",
        "        (b) PTQ using Dynamic Range Quantization\n",
        "        (c) PTQ using Full Integer (int8) Quantization\n",
        "        (d) Evaluate the TF Lite models\n",
        "    [5] Perform Quantization Aware Training (QAT)\n",
        "        (a) Train a TF model through tf.keras\n",
        "        (b) Make it quantization-aware\n",
        "        (c) Quantize the model using Dynamic Range Quantization\n",
        "        (d) Evaluate the TF Lite model performance\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b741ce1e",
      "metadata": {
        "id": "b741ce1e"
      },
      "source": [
        "### Installing the TensorFlow Model Optimisation toolkit\n",
        "\n",
        "You must first install it using pip (comment this out once you have done this).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c331999",
      "metadata": {
        "id": "1c331999"
      },
      "outputs": [],
      "source": [
        "# Install the TF optimization toolkit the first time\n",
        "\n",
        "\n",
        "#!pip install -q tensorflow-model-optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17edd77c-89c5-4259-8fec-83bbdcd64e41",
      "metadata": {
        "id": "17edd77c-89c5-4259-8fec-83bbdcd64e41",
        "outputId": "cde373cf-3629-445c-a2e9-000d20e4dacc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.19.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a77245c1-0175-4b97-9356-29c860a94409",
      "metadata": {
        "id": "a77245c1-0175-4b97-9356-29c860a94409"
      },
      "source": [
        "## 1. Import the data dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abf89547-bdc9-4dae-9a15-23787b84af87",
      "metadata": {
        "id": "abf89547-bdc9-4dae-9a15-23787b84af87"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96ed78b6",
      "metadata": {
        "id": "96ed78b6",
        "outputId": "275f27fd-0a99-41ab-e232-4e47ae657ecc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "# Check that we are using a GPU\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(\"Num GPUs Available: \", len(physical_devices))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "975be93d-8d80-404f-b124-ad257ceab5a2",
      "metadata": {
        "id": "975be93d-8d80-404f-b124-ad257ceab5a2",
        "outputId": "6c91373a-12a8-4806-ac60-98ac90573e0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "print(\"Available devices:\", tf.config.list_physical_devices())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e4fff11-42ed-48a1-9124-6f725b0dee5f",
      "metadata": {
        "id": "5e4fff11-42ed-48a1-9124-6f725b0dee5f"
      },
      "source": [
        "## 2. Generate a TensorFlow Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56141d13",
      "metadata": {
        "id": "56141d13"
      },
      "source": [
        "We'll build a CNN model to classify the 10 fashion item categories from the [FASHION_MNIST dataset](https://www.tensorflow.org/datasets/catalog/fashion_mnist).\n",
        "\n",
        "This training won't take long because you're training the model for just 5 epochs, which trains to about ~90% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6735ca55",
      "metadata": {
        "id": "6735ca55",
        "outputId": "81be07dc-d1c5-4366-9ad7-19a670e02143"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.7557 - loss: 0.6559 - val_accuracy: 0.8740 - val_loss: 0.3353\n",
            "Epoch 2/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - accuracy: 0.8784 - loss: 0.3337 - val_accuracy: 0.8862 - val_loss: 0.3068\n",
            "Epoch 3/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 9ms/step - accuracy: 0.8981 - loss: 0.2769 - val_accuracy: 0.9042 - val_loss: 0.2615\n",
            "Epoch 4/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.9081 - loss: 0.2444 - val_accuracy: 0.8892 - val_loss: 0.2953\n",
            "Epoch 5/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.9182 - loss: 0.2184 - val_accuracy: 0.9087 - val_loss: 0.2453\n"
          ]
        }
      ],
      "source": [
        "# Load Fashion MNIST dataset\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Reshape data for CNN input\n",
        "img_width, img_height = 28, 28\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], img_width, img_height, 1)\n",
        "input_shape = (img_width, img_height, 1)\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 to 1.\n",
        "X_train = X_train.astype(np.float32) / 255.0\n",
        "X_test = X_test.astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    tf.keras.layers.Dropout(rate=0.1), # Randomly disable 10% of neurons\n",
        "    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    tf.keras.layers.Dropout(rate=0.1), # Randomly disable 10% of neurons\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(100, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# Build the model\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.sparse_categorical_crossentropy, # loss function\n",
        "    optimizer=tf.keras.optimizers.Adam(), # optimizer function\n",
        "    metrics=['accuracy'] # reporting metric\n",
        ")\n",
        "\n",
        "\n",
        "# Train the fashion MNIST classification model\n",
        "with tf.device('/CPU:0'):\n",
        "    model.fit(\n",
        "      X_train,\n",
        "      y_train,\n",
        "      epochs=5,\n",
        "      validation_split=0.1,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c5c7951",
      "metadata": {
        "id": "4c5c7951"
      },
      "source": [
        "**Evaluate and save the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a6b8f6",
      "metadata": {
        "id": "d8a6b8f6",
        "outputId": "f86f3fdb-f98c-4617-a3ed-efbaa225556a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9049 - loss: 0.2598\n",
            "Test loss 0.2534, accuracy 90.63%\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"Test loss {:.4f}, accuracy {:.2f}%\".format(score[0], score[1] * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1854b7a",
      "metadata": {
        "id": "f1854b7a",
        "outputId": "183a3fa6-d65c-4af0-8a60-b5b8407b81ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ]
        }
      ],
      "source": [
        "model.save(\"models/new_model.h5\")\n",
        "print(\"Saved model to disk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bef9444e",
      "metadata": {
        "id": "bef9444e"
      },
      "source": [
        "## 3. Convert the trained model to TensorFlow Lite format\n",
        "\n",
        "In the code cell below, convert the model to a **TensorFlow Lite** model and then save this unquantized TFLite model to the ./fashion_mnist_tflite_model directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec207baa",
      "metadata": {
        "id": "ec207baa",
        "outputId": "812d94ea-2012-4088-b819-1fe233d1fedd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpfxhvb4lp/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpfxhvb4lp/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpfxhvb4lp'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  13161036112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13399333904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13483812688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13483813840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13483812496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13483807120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13483807696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13483806160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13483806352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13483805968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1742779117.654095 8917793 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1742779117.654722 8917793 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "2025-03-24 01:18:37.656274: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpfxhvb4lp\n",
            "2025-03-24 01:18:37.656707: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
            "2025-03-24 01:18:37.656712: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpfxhvb4lp\n",
            "2025-03-24 01:18:37.664170: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
            "2025-03-24 01:18:37.725301: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpfxhvb4lp\n",
            "2025-03-24 01:18:37.731669: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 75398 microseconds.\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.load_model('models/new_model.h5')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad0ab56",
      "metadata": {
        "id": "9ad0ab56"
      },
      "source": [
        "It's now a TensorFlow Lite model, but it's still using 32-bit float values for all parameter data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f6acd5b-aee9-4cff-9a6d-f57353e21758",
      "metadata": {
        "id": "5f6acd5b-aee9-4cff-9a6d-f57353e21758",
        "outputId": "674b962d-87b2-4ecb-d95e-aef3b1ddc4e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1825276"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pathlib\n",
        "tflite_models_dir = pathlib.Path(\"./fashion_mnist_tflite_models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Save the unquantized float model:\n",
        "tflite_model_file = tflite_models_dir/\"fashion_mnist_model.tflite\"\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5111131b",
      "metadata": {
        "id": "5111131b"
      },
      "source": [
        "## 4. Post-Training Quantization (PTQ)\n",
        "\n",
        "### Part (a): PTQ using Float 16 Quantization\n",
        "Here for post-training float 16 quantization and then evaluate the file size compared to the unquantized tflite model size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e949f36f",
      "metadata": {
        "id": "e949f36f",
        "outputId": "14529f95-e71c-4683-e306-7d35e74c9e44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpl16dq_ph/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpl16dq_ph/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpl16dq_ph'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  13483818256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5275183888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5275184656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5275182544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5275184464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5275181392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5275181968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5275179856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5275180432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5275185232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1742779230.454108 8917793 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1742779230.454138 8917793 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "2025-03-24 01:20:30.454308: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpl16dq_ph\n",
            "2025-03-24 01:20:30.454748: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
            "2025-03-24 01:20:30.454752: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpl16dq_ph\n",
            "2025-03-24 01:20:30.463695: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
            "2025-03-24 01:20:30.487852: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpl16dq_ph\n",
            "2025-03-24 01:20:30.494378: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 40075 microseconds.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "915704"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert using Float16 Quantization\n",
        "model = tf.keras.models.load_model('models/new_model.h5')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_quant16_model = converter.convert()\n",
        "\n",
        "# Save the quantized 16-bit model:\n",
        "tflite_quant16_model_file = tflite_models_dir/\"fashion_model_quant16.tflite\"\n",
        "tflite_quant16_model_file.write_bytes(tflite_quant16_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a76ac94",
      "metadata": {
        "id": "7a76ac94"
      },
      "source": [
        "**Evaluate the reduction in size of the model** - how much smaller is the Quantized 16-bit model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f04e939",
      "metadata": {
        "id": "2f04e939",
        "outputId": "8aa04487-8069-4b3c-e0f1-cf77035888dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Float model in Mb: 1.7407188415527344\n",
            "Quantized 16-bit model in Mb: 0.8732833862304688\n",
            "Compression ratio: 1.9933035129255743\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Float model in Mb:\", os.path.getsize(tflite_model_file) / float(2**20))\n",
        "print(\"Quantized 16-bit model in Mb:\", os.path.getsize(tflite_quant16_model_file) / float(2**20))\n",
        "print(\"Compression ratio:\", os.path.getsize(tflite_model_file)/os.path.getsize(tflite_quant16_model_file))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dca53ec4-ec81-4a2e-ba77-4b52fbe09f92",
      "metadata": {
        "id": "dca53ec4-ec81-4a2e-ba77-4b52fbe09f92",
        "outputId": "5639e0ee-2417-4b67-c14a-86f64408c1ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compression ratio: 1.9933035129255743\n"
          ]
        }
      ],
      "source": [
        "compression_ratio = os.path.getsize(tflite_model_file) / os.path.getsize(tflite_quant16_model_file)\n",
        "print(f\"Compression ratio: {compression_ratio:}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94ae4ad9",
      "metadata": {
        "id": "94ae4ad9"
      },
      "source": [
        "### Part (b): PTQ using Dynamic Range Quantization\n",
        "Next quantize the original model dynamically to change the model weight and activations from float to int8 format. Convert the model using **Dynamic Range Quantization** and evaluate the model file size reduction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0db93f09",
      "metadata": {
        "id": "0db93f09",
        "outputId": "333c0f3d-46b5-427a-fbff-4aa5b2802959"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpyo6g2f_f/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpyo6g2f_f/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpyo6g2f_f'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  13483806544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13483812304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13483806160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13483808080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13483808272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13483807888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13483807504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13483803280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13483803472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13483804816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1742779424.176053 8917793 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1742779424.176329 8917793 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "2025-03-24 01:23:44.177692: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpyo6g2f_f\n",
            "2025-03-24 01:23:44.178346: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
            "2025-03-24 01:23:44.178351: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpyo6g2f_f\n",
            "2025-03-24 01:23:44.184559: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
            "2025-03-24 01:23:44.249420: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpyo6g2f_f\n",
            "2025-03-24 01:23:44.255311: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 77623 microseconds.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "469432"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "model = tf.keras.models.load_model('models/new_model.h5')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "# Save the quantized model:\n",
        "tflite_quant_model_file = tflite_models_dir/\"fashion_model_quant.tflite\"\n",
        "tflite_quant_model_file.write_bytes(tflite_quant_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8446084",
      "metadata": {
        "id": "b8446084"
      },
      "source": [
        " **Evaluate the reduction in size of the model** - how much smaller is the Quantized model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "176f70b4",
      "metadata": {
        "id": "176f70b4",
        "outputId": "966030f1-255f-4709-ff7e-049b05673c47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Float model in Mb: 1.7407188415527344\n",
            "Quantized model in Mb: 0.44768524169921875\n",
            "Compression ratio: 3.8882649670239777\n"
          ]
        }
      ],
      "source": [
        "print(\"Float model in Mb:\", os.path.getsize(tflite_model_file) / float(2**20))\n",
        "print(\"Quantized model in Mb:\", os.path.getsize(tflite_quant_model_file) / float(2**20))\n",
        "print(\"Compression ratio:\", os.path.getsize(tflite_model_file)/os.path.getsize(tflite_quant_model_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f102c970",
      "metadata": {
        "id": "f102c970",
        "outputId": "a82dc538-43ae-471a-c5d1-871f97701436"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "90.64\n"
          ]
        }
      ],
      "source": [
        "# Load TFLite model and allocate tensors.\n",
        "interpreter = \\\n",
        "tf.lite.Interpreter(model_path=str(tflite_quant_model_file))\n",
        "interpreter.allocate_tensors()\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "# Test model on some input data.\n",
        "input_shape = input_details[0]['shape']\n",
        "acc=0\n",
        "for i in range(len(X_test)):\n",
        "    input_data = X_test[i].reshape(input_shape)\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "    interpreter.invoke()\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "#    if(np.argmax(output_data) == np.argmax(test_labels[i])):\n",
        "    if(np.argmax(output_data) == y_test[i]):\n",
        "        acc+=1\n",
        "acc = acc/len(X_test)\n",
        "print(acc*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f54f0646",
      "metadata": {
        "id": "f54f0646"
      },
      "source": [
        "### Part (c): PTQ using Full Integer (int8) Quantization\n",
        "Convert the original model to satisfy **full integer quantization** so that everything is converted (including activations) from float32 into int8 format. Evaluate the model file size reduction. Note you will need to use the OPTIMIZE_FOR_SIZE option by using a small representative dataset of the model and also make sure the input and output tensors are in int8 format."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "607223a6",
      "metadata": {
        "id": "607223a6"
      },
      "source": [
        "**Check that the input and output tensors are in int8 format**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "106b9677",
      "metadata": {
        "id": "106b9677",
        "outputId": "3a066a83-58d5-43fe-ee82-d93a1a17f311"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpou_oz0db/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpou_oz0db/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpou_oz0db'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  13399334672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13399330832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5323852112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5323853264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5323851920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5323854416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5323853840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5323855376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5323855184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5323856144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n",
            "W0000 00:00:1742779992.964734 8917793 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1742779992.964773 8917793 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "2025-03-24 01:33:12.964994: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpou_oz0db\n",
            "2025-03-24 01:33:12.965870: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
            "2025-03-24 01:33:12.965877: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpou_oz0db\n",
            "2025-03-24 01:33:12.974815: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
            "2025-03-24 01:33:13.005589: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpou_oz0db\n",
            "2025-03-24 01:33:13.013664: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 48676 microseconds.\n",
            "2025-03-24 01:33:13.229626: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "fully_quantize: 0, inference_type: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input tensor type: <class 'numpy.int8'>\n",
            "Output tensor type: <class 'numpy.int8'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6, input_inference_type: INT8, output_inference_type: INT8\n",
            "/opt/anaconda3/lib/python3.12/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load and preprocess Fashion MNIST data\n",
        "fashion_mnist_train, _ = tf.keras.datasets.fashion_mnist.load_data()\n",
        "images = tf.cast(fashion_mnist_train[0], tf.float32) / 255.0  # Normalize to [0, 1]\n",
        "images = tf.expand_dims(images, axis=-1)  # Add channel dimension: (28, 28, 1)\n",
        "\n",
        "# Create dataset with batch size 1\n",
        "mnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\n",
        "\n",
        "# Define representative dataset generator for calibration\n",
        "def representative_data_gen():\n",
        "    for input_value in mnist_ds.take(100):  # Use 100 samples for calibration\n",
        "        yield [tf.cast(input_value, tf.float32)]\n",
        "\n",
        "# Load the Keras model\n",
        "model = tf.keras.models.load_model('models/new_model.h5')\n",
        "\n",
        "# Create TFLite converter\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# Use updated optimization setting\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Assign the representative dataset\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "# Force full integer quantization (including inputs/outputs)\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "# Convert the model\n",
        "tflite_fullquant_model = converter.convert()\n",
        "\n",
        "# Save the quantized model\n",
        "with open('models/new_model_full_integer_quant.tflite', 'wb') as f:\n",
        "    f.write(tflite_fullquant_model)\n",
        "\n",
        "# Verify input/output types\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_fullquant_model)\n",
        "input_type = interpreter.get_input_details()[0]['dtype']\n",
        "output_type = interpreter.get_output_details()[0]['dtype']\n",
        "print('Input tensor type:', input_type)\n",
        "print('Output tensor type:', output_type)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b6e5221-0526-4bc1-851c-4b9017351605",
      "metadata": {
        "id": "9b6e5221-0526-4bc1-851c-4b9017351605",
        "outputId": "d7a0a5c8-fb27-4494-e297-6f75b115c69c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpxs4nm5tk/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpxs4nm5tk/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpxs4nm5tk'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  5235688464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5235691344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5235689808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5235692688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5235690768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5235693840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5235693264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5235694800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5235694608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5235695568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n",
            "W0000 00:00:1742780438.048031 8917793 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1742780438.049056 8917793 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "2025-03-24 01:40:38.050102: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpxs4nm5tk\n",
            "2025-03-24 01:40:38.050557: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
            "2025-03-24 01:40:38.050561: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpxs4nm5tk\n",
            "2025-03-24 01:40:38.059620: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
            "2025-03-24 01:40:38.128600: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmpxs4nm5tk\n",
            "2025-03-24 01:40:38.134775: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 84678 microseconds.\n",
            "2025-03-24 01:40:39.099185: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "472568"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def representative_data_gen():\n",
        "  for input_value in tf.data.Dataset.from_tensor_slices(X_train).batch(1).take(100):\n",
        "    yield [input_value]\n",
        "\n",
        "model = tf.keras.models.load_model('models/new_model.h5')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "tflite_fullquant_model = converter.convert()\n",
        "\n",
        "# Saving the fully-quantized 8-bit model:\n",
        "tflite_fullquant_model_file = tflite_models_dir/\"mnist_model_fullquant.tflite\"\n",
        "tflite_fullquant_model_file.write_bytes(tflite_fullquant_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0aaa4a8",
      "metadata": {
        "id": "e0aaa4a8"
      },
      "source": [
        " **Evaluate the reduction in size of the model** - how much smaller is the Quantized model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa8c9ed9",
      "metadata": {
        "id": "aa8c9ed9",
        "outputId": "8283c034-f981-4714-df74-46faa90adc5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input:  <class 'numpy.uint8'>\n",
            "output:  <class 'numpy.uint8'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        }
      ],
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=tflite_fullquant_model)\n",
        "input_type = interpreter.get_input_details()[0]['dtype']\n",
        "print('input: ', input_type)\n",
        "output_type = interpreter.get_output_details()[0]['dtype']\n",
        "print('output: ', output_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f038fbe-ae7e-4f88-8141-2a62a62b2f41",
      "metadata": {
        "id": "7f038fbe-ae7e-4f88-8141-2a62a62b2f41",
        "outputId": "ce4c3da6-2ca4-46b4-e994-e8abda811692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Float model in Mb: 1.7407188415527344\n",
            "Full Integer Quantized model in Mb: 0.45067596435546875\n",
            "Compression ratio: 3.862462121853363\n"
          ]
        }
      ],
      "source": [
        "print(\"Float model in Mb:\", os.path.getsize(tflite_model_file) / float(2**20))\n",
        "print(\"Full Integer Quantized model in Mb:\", os.path.getsize(tflite_fullquant_model_file) / float(2**20))\n",
        "print(\"Compression ratio:\", os.path.getsize(tflite_model_file)/os.path.getsize(tflite_fullquant_model_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f33fbc8c",
      "metadata": {
        "id": "f33fbc8c"
      },
      "source": [
        "### Part (d):  Evaluate the TF Lite models on all images\n",
        "\n",
        "In this section, evaluate the four TF Lite models by running inference using the TensorFlow Lite [`Interpreter`](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter) to compare the model accuracies. First, build a **run_tflite_model()** function to run inference on a TF Lite model and then an **evaluate_model()** function to evaluate the TF Lite model on all images in the X_test dataset.\n",
        "\n",
        "**Evaluate the model performance for these models** by reporting on the model accuracies.\n",
        "1. Float model (Unquantized)\n",
        "2. 16-bit quantized model\n",
        "3. Initial quantized 8-bit model\n",
        "4. Fully quantized 8-bit model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "521bf4ba",
      "metadata": {
        "id": "521bf4ba"
      },
      "outputs": [],
      "source": [
        "def run_tflite_model(tflite_file, test_image_indices):\n",
        "  global test_images\n",
        "\n",
        "  # Initialize the interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
        "  for i, test_image_index in enumerate(test_image_indices):\n",
        "    test_image = X_test[test_image_index]\n",
        "    test_label = y_test[test_image_index]\n",
        "\n",
        "    # Check if the input type is quantized, then rescale input data to uint8\n",
        "    if input_details['dtype'] == np.uint8:\n",
        "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "      test_image = test_image / input_scale + input_zero_point\n",
        "\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
        "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "\n",
        "    predictions[i] = output.argmax()\n",
        "\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "060a5db7-917a-4efc-ad1c-abe92d82bcf2",
      "metadata": {
        "id": "060a5db7-917a-4efc-ad1c-abe92d82bcf2"
      },
      "outputs": [],
      "source": [
        "# Helper function to evaluate a TFLite model on all images\n",
        "def evaluate_model(tflite_file, model_type):\n",
        "  global test_images\n",
        "  global test_labels\n",
        "\n",
        "  test_image_indices = range(X_test.shape[0])\n",
        "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
        "\n",
        "  accuracy = (np.sum(y_test== predictions) * 100) / len(X_test)\n",
        "\n",
        "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
        "      model_type, accuracy, len(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "606b4d3d",
      "metadata": {
        "id": "606b4d3d"
      },
      "source": [
        "1. Evaluate the float model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4776ce1e",
      "metadata": {
        "id": "4776ce1e",
        "outputId": "f16159c2-e650-4063-8362-35bb83f48c97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Float model accuracy is 90.6300% (Number of test samples=10000)\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(tflite_model_file, model_type=\"Float\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbc534c5",
      "metadata": {
        "id": "fbc534c5"
      },
      "source": [
        "2. Evaluate the 16-bit quantized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bda7ec42",
      "metadata": {
        "id": "bda7ec42",
        "outputId": "8ee09c77-664a-44da-eccf-b291208e1648"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16-bit Quantized model accuracy is 90.6300% (Number of test samples=10000)\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(tflite_quant16_model_file, model_type=\"16-bit Quantized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9154c7b3",
      "metadata": {
        "id": "9154c7b3"
      },
      "source": [
        "3. Evaluate the initial quantized 8-bit model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fdb55ab",
      "metadata": {
        "id": "2fdb55ab",
        "outputId": "54064583-8262-44dc-e4b2-97fc081b3dd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantized model accuracy is 90.6400% (Number of test samples=10000)\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(tflite_quant_model_file, model_type=\"Quantized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1967e30a",
      "metadata": {
        "id": "1967e30a"
      },
      "source": [
        "4. Evaluate the fully quantized 8-bit integer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20c4d68f",
      "metadata": {
        "id": "20c4d68f",
        "outputId": "37144636-b6d8-47c9-efc3-1eb7e41ba4bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fully Quantized model accuracy is 90.5900% (Number of test samples=10000)\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(tflite_fullquant_model_file, model_type=\"Fully Quantized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "844b8c7c",
      "metadata": {
        "id": "844b8c7c"
      },
      "source": [
        "## 5. Quantization-Aware Training (QAT)\n",
        "\n",
        "QAT models quantization during training and typically provides higher accuracies as compared to post-training quantization.\n",
        "Generally, QAT is a three-step process:\n",
        "\n",
        "    (a) Train a regular model through tf.keras\n",
        "        YOU MAY HAVE TO 'import tf_keras as keras' and use model = keras.Sequential([...]) format.\n",
        "    (b) Make it quantization-aware by applying the related API, allowing it to learn those loss-robust parameters.\n",
        "    (c) Quantize the model use one of the approaches mentioned above and analyse performance\n",
        "\n",
        "\n",
        "### **Part (a)**: Train a model for the FASHION MNIST dataset again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7917f6d1",
      "metadata": {
        "id": "7917f6d1",
        "outputId": "b4631da2-d31c-44c6-ef17-a9d322634a49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 0.3995 - accuracy: 0.8594 - val_loss: 0.3182 - val_accuracy: 0.8820\n",
            "Epoch 2/5\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 0.2707 - accuracy: 0.9025 - val_loss: 0.2743 - val_accuracy: 0.9017\n",
            "Epoch 3/5\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 0.2253 - accuracy: 0.9168 - val_loss: 0.2522 - val_accuracy: 0.9103\n",
            "Epoch 4/5\n",
            "1688/1688 [==============================] - 10s 6ms/step - loss: 0.1911 - accuracy: 0.9297 - val_loss: 0.2543 - val_accuracy: 0.9107\n",
            "Epoch 5/5\n",
            "1688/1688 [==============================] - 10s 6ms/step - loss: 0.1662 - accuracy: 0.9389 - val_loss: 0.2363 - val_accuracy: 0.9170\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x13ffb11f0>"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load and preprocess Fashion MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Add channel dimension: (28, 28, 1)\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]\n",
        "\n",
        "# Define a simple CNN model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Input(shape=(28, 28, 1)),\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, validation_split=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fbc778d",
      "metadata": {
        "id": "8fbc778d"
      },
      "source": [
        "### Part (b): Make the model quantization aware\n",
        "Hint: Use q_aware_model = quantize_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed3ca15e-4c72-4bbe-8681-0861d98cfa87",
      "metadata": {
        "id": "ed3ca15e-4c72-4bbe-8681-0861d98cfa87",
        "outputId": "e45d9649-55cd-421a-b08f-39d07d1cb059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-core in /opt/anaconda3/lib/python3.12/site-packages (0.1.7)\n",
            "Requirement already satisfied: absl-py in /Users/siddy/.local/lib/python3.12/site-packages (from keras-core) (1.4.0)\n",
            "Requirement already satisfied: numpy in /Users/siddy/.local/lib/python3.12/site-packages (from keras-core) (1.26.4)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras-core) (13.7.1)\n",
            "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras-core) (0.0.8)\n",
            "Requirement already satisfied: h5py in /opt/anaconda3/lib/python3.12/site-packages (from keras-core) (3.11.0)\n",
            "Requirement already satisfied: dm-tree in /Users/siddy/.local/lib/python3.12/site-packages (from keras-core) (0.1.9)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from dm-tree->keras-core) (23.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /opt/anaconda3/lib/python3.12/site-packages (from dm-tree->keras-core) (1.14.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras-core) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras-core) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras-core) (0.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-core\n",
        "import tf_keras as keras\n",
        "import tensorflow_model_optimization as tfmot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05faf889",
      "metadata": {
        "id": "05faf889",
        "outputId": "9a633746-0622-4d05-de93-d31a18381b2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer (QuantizeLa  (None, 28, 28, 1)         3         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " quant_conv2d (QuantizeWrap  (None, 26, 26, 32)        387       \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_max_pooling2d (Quant  (None, 13, 13, 32)        1         \n",
            " izeWrapperV2)                                                   \n",
            "                                                                 \n",
            " quant_flatten (QuantizeWra  (None, 5408)              1         \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_dense (QuantizeWrapp  (None, 128)               692357    \n",
            " erV2)                                                           \n",
            "                                                                 \n",
            " quant_dense_1 (QuantizeWra  (None, 10)                1295      \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 694044 (2.65 MB)\n",
            "Trainable params: 693962 (2.65 MB)\n",
            "Non-trainable params: 82 (328.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# q_aware stands for for quantization aware.\n",
        "q_aware_model = quantize_model(model)\n",
        "\n",
        "# `quantize_model` requires a recompile.\n",
        "q_aware_model.compile(optimizer='adam',\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "q_aware_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7e475cc",
      "metadata": {
        "id": "a7e475cc"
      },
      "source": [
        "#### Retrain the quantization aware model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1661628c",
      "metadata": {
        "id": "1661628c",
        "outputId": "f5dcc4e8-1bdc-4263-f819-236f29358ba3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1688/1688 [==============================] - 12s 7ms/step - loss: 0.1435 - accuracy: 0.9469 - val_loss: 0.2394 - val_accuracy: 0.9192\n",
            "Epoch 2/5\n",
            "1688/1688 [==============================] - 14s 8ms/step - loss: 0.1198 - accuracy: 0.9556 - val_loss: 0.2684 - val_accuracy: 0.9147\n",
            "Epoch 3/5\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.1009 - accuracy: 0.9625 - val_loss: 0.2621 - val_accuracy: 0.9213\n",
            "Epoch 4/5\n",
            "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0846 - accuracy: 0.9692 - val_loss: 0.2736 - val_accuracy: 0.9177\n",
            "Epoch 5/5\n",
            "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0724 - accuracy: 0.9740 - val_loss: 0.2870 - val_accuracy: 0.9228\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x321cfe3c0>"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q_aware_model.fit(\n",
        "  x_train,\n",
        "  y_train,\n",
        "  epochs=5,\n",
        "  validation_split=0.1\n",
        "  #validation_data=(test_images, test_labels)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "080913b4",
      "metadata": {
        "id": "080913b4"
      },
      "source": [
        "#### Compare the accuracy of the baseline model to the new QAT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "521e4ca6",
      "metadata": {
        "id": "521e4ca6",
        "outputId": "cf79acde-4229-443b-cc4c-5565efca5a49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2497 - accuracy: 0.9130\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3167 - accuracy: 0.9167\n",
            "\n",
            "-------------------------------------------------------------\n",
            "Baseline test accuracy: 91.29999876022339\n",
            "Quant test accuracy: 91.6700005531311\n"
          ]
        }
      ],
      "source": [
        "_, baseline_model_accuracy = model.evaluate(\n",
        "    x_test, y_test, verbose=1)\n",
        "\n",
        "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
        "    x_test, y_test, verbose=1)\n",
        "print(\"\\n-------------------------------------------------------------\")\n",
        "print('Baseline test accuracy:', baseline_model_accuracy*100)\n",
        "print('Quant test accuracy:', q_aware_model_accuracy*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d1242bf",
      "metadata": {
        "id": "3d1242bf"
      },
      "source": [
        "#### Fine tune with QAT on a subset of the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "707dd9da",
      "metadata": {
        "id": "707dd9da",
        "outputId": "53b15df8-70a7-4995-f526-3a18bb61e814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 0.0552 - accuracy: 0.9844 - val_loss: 0.0422 - val_accuracy: 0.9900\n",
            "Epoch 2/5\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.0212 - accuracy: 0.9956 - val_loss: 0.0450 - val_accuracy: 0.9900\n",
            "Epoch 3/5\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0573 - val_accuracy: 0.9700\n",
            "Epoch 4/5\n",
            "29/29 [==============================] - 1s 17ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0463 - val_accuracy: 0.9800\n",
            "Epoch 5/5\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9800\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x321e4a240>"
            ]
          },
          "execution_count": 174,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q_aware_model.fit(\n",
        "  x_train[:1000],\n",
        "  y_train[:1000],\n",
        "  epochs=5,\n",
        "  validation_split=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d218cb3",
      "metadata": {
        "id": "3d218cb3"
      },
      "source": [
        "#### Re-evaluate the model accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2db3c9d",
      "metadata": {
        "id": "b2db3c9d",
        "outputId": "c7d9ca14-6cdc-443e-c850-0530ab1e4ace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2497 - accuracy: 0.9130\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3337 - accuracy: 0.9172\n",
            "\n",
            "-------------------------------------------------------------\n",
            "Baseline test accuracy: 91.29999876022339\n",
            "Quant test accuracy: 91.72000288963318\n"
          ]
        }
      ],
      "source": [
        "_, baseline_model_accuracy = model.evaluate(\n",
        "    x_test, y_test, verbose=1)\n",
        "\n",
        "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
        "    x_test, y_test, verbose=1)\n",
        "print(\"\\n-------------------------------------------------------------\")\n",
        "print('Baseline test accuracy:', baseline_model_accuracy*100)\n",
        "print('Quant test accuracy:', q_aware_model_accuracy*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87f5d686",
      "metadata": {
        "id": "87f5d686"
      },
      "source": [
        "#### Save the QAT model to the ./models directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b36edf7f",
      "metadata": {
        "id": "b36edf7f",
        "outputId": "d408d5a0-2cf6-4b83-b0d6-c2e5658c2dad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "#Save the entire model into a qat_model.h5 file\n",
        "model.save(\"models/qat_model.h5\")\n",
        "print(\"Saved model to disk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f4a6b9",
      "metadata": {
        "id": "e4f4a6b9"
      },
      "source": [
        "### Part (c): Convert the model to TF Lite format  using Dynamic Range Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6cf8e44",
      "metadata": {
        "scrolled": true,
        "id": "e6cf8e44",
        "outputId": "28ee5dda-6a1a-40fc-da3b-23e4d2bcf025"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
            "  warnings.warn(\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmptc5zdifo/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmptc5zdifo/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmptc5zdifo'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_1')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  5365444688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  6293380880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  6293388368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  6293386256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13452606544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13452601552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1742781763.862796 8917793 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1742781763.863045 8917793 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "2025-03-24 02:02:43.864358: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmptc5zdifo\n",
            "2025-03-24 02:02:43.865077: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
            "2025-03-24 02:02:43.865082: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmptc5zdifo\n",
            "2025-03-24 02:02:43.870760: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
            "2025-03-24 02:02:43.965772: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/d3/gzz38_ss2ylbt0sgg3k_jkym0000gn/T/tmptc5zdifo\n",
            "2025-03-24 02:02:43.970689: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 106332 microseconds.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "699704"
            ]
          },
          "execution_count": 180,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = tf.keras.models.load_model('models/qat_model.h5')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quantaware_model = converter.convert()\n",
        "\n",
        "# Saving the quantized aware model:\n",
        "tflite_quantaware_model_file = tflite_models_dir/\"mnist_model_quantaware.tflite\"\n",
        "tflite_quantaware_model_file.write_bytes(tflite_quantaware_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53e32eae",
      "metadata": {
        "id": "53e32eae"
      },
      "source": [
        "**Evaluate the reduction in size of the model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f50137a5",
      "metadata": {
        "id": "f50137a5",
        "outputId": "ff4f845c-11c8-45fd-8566-76d6495763db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Float model in Mb: 1.7407188415527344\n",
            "Quantized aware (QAT) model in Mb: 0.6672897338867188\n",
            "Compression ratio: 2.6086402250094327\n"
          ]
        }
      ],
      "source": [
        "print(\"Float model in Mb:\", os.path.getsize(tflite_model_file) / float(2**20))\n",
        "print(\"Quantized aware (QAT) model in Mb:\", os.path.getsize(tflite_quantaware_model_file) / float(2**20))\n",
        "print(\"Compression ratio:\", os.path.getsize(tflite_model_file)/os.path.getsize(tflite_quantaware_model_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82990f54",
      "metadata": {
        "id": "82990f54"
      },
      "source": [
        "### Part (d): Evaluate the TF Lite QAT model accuracy\n",
        "Hint: Use the intrepreter evaluate_model() function to get the accuracy result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "809710e4",
      "metadata": {
        "id": "809710e4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  prediction_fashion_type = []\n",
        "  for i, test_image in enumerate(x_test):\n",
        "    if i % 1000 == 0:\n",
        "      print('Evaluated on {n} results so far.'.format(n=i))\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    fashion_type = np.argmax(output()[0])\n",
        "    prediction_fashion_type.append(fashion_type)\n",
        "\n",
        "  print('\\n')\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_fashion_type = np.array(prediction_fashion_type)\n",
        "  accuracy = (prediction_fashion_type == y_test).mean()\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29a6132a",
      "metadata": {
        "id": "29a6132a",
        "outputId": "a4ca0cb4-ca38-4d91-a08c-2f9673711812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluated on 0 results so far.\n",
            "Evaluated on 1000 results so far.\n",
            "Evaluated on 2000 results so far.\n",
            "Evaluated on 3000 results so far.\n",
            "Evaluated on 4000 results so far.\n",
            "Evaluated on 5000 results so far.\n",
            "Evaluated on 6000 results so far.\n",
            "Evaluated on 7000 results so far.\n",
            "Evaluated on 8000 results so far.\n",
            "Evaluated on 9000 results so far.\n",
            "\n",
            "\n",
            "Quant TFLite test_accuracy: 0.9134\n",
            "Quant TF test accuracy: 0.9172000288963318\n"
          ]
        }
      ],
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=tflite_quantaware_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "print('Quant TFLite test_accuracy:', test_accuracy)\n",
        "print('Quant TF test accuracy:', q_aware_model_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9ab6f8c",
      "metadata": {
        "id": "c9ab6f8c"
      },
      "source": [
        "## <span style='color: red;'>Comment on the results:</span> ##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b49b34c",
      "metadata": {
        "id": "9b49b34c"
      },
      "source": [
        "---\n",
        "\n",
        "Through this exercise, I explored various quantization techniques and their effects on a convolutional neural network (CNN) trained on the Fashion MNIST dataset. Initially, I developed a baseline float32 model, which achieved a test accuracy of **90.60%** with a model size of approximately **1.74 MB**. This served as a reference point for evaluating how different quantization strategies affect both model size and accuracy.\n",
        "\n",
        "I first applied **Float16 post-training quantization**, which reduced the model size by almost half to **0.87 MB** (a **1.99x compression ratio**) while retaining the same accuracy of **90.63%**. This demonstrated that converting to half precision has negligible impact on accuracy, yet offers a significant reduction in memory footprint.\n",
        "\n",
        "Then, I explored **dynamic range quantization** (weights in int8 but float inputs/outputs). This approach shrank the model size further to **0.45 MB**, achieving a **3.89x compression ratio** with an accuracy of **90.64%**. Similarly, **full integer quantization** using uint8 for weights, inputs, and outputs, resulted in **0.45 MB size** and **90.59% accuracy**. Both methods confirmed that quantization can **significantly compress models** with **minimal accuracy trade-off**.\n",
        "\n",
        "To maximize efficiency while preserving model performance, I implemented **Quantization-Aware Training (QAT)**. This method allowed the model to adapt to lower precision during training, and the results were impressive. The QAT-trained model achieved **91.72% accuracy**, even slightly exceeding the float32 baseline. When converted to TFLite, the quantized QAT model maintained **91.34% accuracy** and had a compressed size of **0.67 MB** (a **2.61x compression ratio**). Fine-tuning on just a **subset of training data** during QAT further enhanced accuracy, highlighting the method's adaptability.\n",
        "\n",
        "---\n",
        "\n",
        "### **Observations**\n",
        "- **Accuracy Stability**: Across all quantization types, **accuracy remained above 90.5%**, validating that quantization has minimal impact on model performance, especially on balanced datasets like Fashion MNIST.\n",
        "- **Compression vs. Performance**: **Float16 quantization** offered a moderate compression with **no accuracy loss**, while **full integer quantization** provided **maximum compression** with **slightly reduced accuracy**.\n",
        "- **QAT Effectiveness**: Among all methods, **Quantization-Aware Training was the most effective**, achieving **near float32 accuracy** after quantization and offering a good balance between compression and performance.\n",
        "- **Trade-off Understanding**: I observed that **post-training quantization is quicker and easier**, but QAT is more suitable when **accuracy retention is critical**, especially for models with tighter performance margins.\n",
        "\n",
        "---\n",
        "\n",
        "This exercise gave me valuable insights into the **practical trade-offs** between **model size, accuracy, and computational efficiency**. I learned how different quantization techniques serve different deployment needs, from lightweight models for microcontrollers to high-performance models for mobile devices. More importantly, I experienced how **Quantization-Aware Training can yield optimized models** without sacrificing accuracy, and how **fine-tuning even on smaller datasets can enhance performance**. Overall, this hands-on experience strengthened my understanding of how to prepare models for **real-world, resource-constrained environments** through quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79ad6625-23d6-45cd-828b-c53375d713e1",
      "metadata": {
        "id": "79ad6625-23d6-45cd-828b-c53375d713e1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}