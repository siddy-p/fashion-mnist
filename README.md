# Model Compression with TensorFlow/Keras: Pruning & Quantization

This repo contains two Jupyter notebooks showing end‑to‑end **model compression** on classic vision datasets using **TensorFlow/Keras** and the **TensorFlow Model Optimization Toolkit (TFMOT)**:

- **`Pruning_24192929.ipynb`** – Structured workflow for pruning a Keras model, measuring sparsity and accuracy, and exporting the pruned model.
- **`Quantization_24192929.ipynb`** – Quantization workflows including **Quantization‑Aware Training (QAT)** and TF‑Lite conversion to **INT8** (full‑integer). Produces a deployable `.tflite` file.

---

## 🧱 What’s inside

```
.
├── Pruning_24192929.ipynb
├── Quantization_24192929.ipynb
└── models/
    └── new_model_full_integer_quant.tflite   # generated by the quantization notebook
```

> **Datasets used in the notebooks:** `MNIST` (and in places `Fashion‑MNIST`). These are downloaded automatically via `tf.keras.datasets`—no manual download needed.

---

## ✅ Verified environment (from notebooks)

- **Python:** 3.12
- **Kernels:** `cpu_env`, `conda-base-py`
- **Frameworks imported:** `tensorflow`, `keras / tf_keras`, `tensorflow_model_optimization (tfmot)`, `scikit‑learn`
- **Common libs:** `numpy`, `matplotlib`, `pandas`
- **Hardware:** runs on CPU; GPU/CUDA is optional (notebooks detect/use it when available).

---

## 📦 Installation

Create a fresh environment (recommended), then install dependencies:

```bash
pip install   "tensorflow>=2.12"   tensorflow-model-optimization   numpy matplotlib pandas scikit-learn
```

> If you’re on Keras 3 and see `tf_keras` imports, install the compatibility layer:
>
> ```bash
> pip install keras==3.* keras-core
> ```

---

## ▶️ How to run

1. **Clone** your repository and **enter** it:
   ```bash
   git clone <YOUR_REPO_URL>.git
   cd <YOUR_REPO_DIR>
   ```

2. **(Optional) Create env** and install packages (see above).

3. **Launch Jupyter** and open either notebook:
   ```bash
   jupyter notebook
   ```

4. **Run all cells** in:
   - `Pruning_24192929.ipynb` – pruning workflow
   - `Quantization_24192929.ipynb` – QAT + TF‑Lite INT8 conversion

> The quantization notebook writes a TF‑Lite model to:
>
> `./models/new_model_full_integer_quant.tflite`

---

## 🧪 Notebook details

### 1) Pruning (`Pruning_24192929.ipynb`)

**Main steps (as implemented in the notebook):**
- Load and preprocess **MNIST / Fashion‑MNIST** with `tf.keras.datasets`.
- Build and train a baseline **Keras** model.
- Evaluate baseline **accuracy**.
- Configure pruning using **TFMOT** (`tfmot.sparsity.keras`): define pruning schedule and callbacks.
- **Apply pruning**, train/fine‑tune, and **measure sparsity** / size effects.
- (Typical) Strip pruning wrappers and **export** the final pruned model/artifacts.

**Key libraries used:** `tensorflow`, `keras (tf_keras)`, `tensorflow_model_optimization`, `numpy`, `matplotlib`, `pandas`, `scikit‑learn`.

**Metrics tracked:** accuracy, sparsity/size (and optionally precision if computed).


### 2) Quantization (`Quantization_24192929.ipynb`)

**Main steps (as implemented in the notebook):**
- Load and preprocess **MNIST**.
- Train or load a floating‑point Keras model.
- Configure **QAT** using **TFMOT** (`tfmot.quantization.keras`).
- Fine‑tune the QAT model and evaluate **accuracy**.
- **Convert to TF‑Lite** with **full‑integer (INT8)** quantization (with calibration dataset where needed).
- **Export**: `models/new_model_full_integer_quant.tflite`.
- (Optional) Evaluate the TF‑Lite model accuracy via an interpreter loop.

**Key libraries used:** `tensorflow`, `tensorflow_model_optimization`, `numpy`, `matplotlib`.

**Metrics tracked:** accuracy, model size (before/after), and (optionally) precision.


---

## 📊 Record your results

Use this table to track your outcomes (fill in after running the notebooks):

| Experiment | Dataset        | Baseline Acc. | Compressed Acc. | Size (MB) Before | Size (MB) After | Notes |
|-----------:|----------------|---------------:|-----------------:|------------------:|----------------:|-------|
| Pruning    | MNIST / F‑MNIST|                |                  |                   |                 |       |
| QAT (INT8) | MNIST          |                |                  |                   |                 |       |

---

## 💡 Tips

- For reproducible training, set random seeds (`tf.random.set_seed`, `np.random.seed`).
- Start with moderate pruning sparsity, then increase gradually to limit accuracy drop.
- For **INT8** conversion quality, ensure representative samples for calibration and (ideally) a few more fine‑tuning epochs during QAT.
- Test the `.tflite` model on the target device to verify latency and accuracy.

---

## 📝 License

MIT — feel free to use and adapt.
